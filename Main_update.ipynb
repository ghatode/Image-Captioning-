{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAVbfSyjTgd7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, RepeatVector, TimeDistributed, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import pickle\n",
        "\n",
        "# Set paths (adjust as needed)\n",
        "IMAGES_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flicker8k_Dataset/'\n",
        "CAPTIONS_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr8k.token.txt'\n",
        "TRAIN_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "TEST_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "GLOVE_PATH = '/data/glove.6B.100d.txt'\n",
        "\n",
        "# Load captions\n",
        "captions = open(CAPTIONS_PATH, 'r').read().split(\"\\n\")\n",
        "tokens = {}\n",
        "for line in captions:\n",
        "    if not line:\n",
        "        continue\n",
        "    temp = line.split(\"#\")\n",
        "    if len(temp) < 2:\n",
        "        continue\n",
        "    img, cap = temp[0], temp[1][2:]\n",
        "    if img in tokens:\n",
        "        tokens[img].append(cap)\n",
        "    else:\n",
        "        tokens[img] = [cap]\n",
        "\n",
        "# Load train and test image names\n",
        "x_train = open(TRAIN_PATH, 'r').read().split(\"\\n\")\n",
        "x_test = open(TEST_PATH, 'r').read().split(\"\\n\")\n",
        "x_train = [x for x in x_train if x]\n",
        "x_test = [x for x in x_test if x]\n",
        "print(f\"Number of Training Images: {len(x_train)}\")\n",
        "print(f\"Number of Test Images: {len(x_test)}\")\n",
        "\n",
        "# Preprocess images for ResNet50\n",
        "def preprocess_input_resnet(img):\n",
        "    from tensorflow.keras.applications.resnet50\n",
        "    img = resnet50.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def preprocess_image(img_path, augment=False):\n",
        "    img = image.load_img(img_path, target_size=(224, 224, 3))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    if augment:\n",
        "        aug = ImageDataGenerator(\n",
        "            rotation_range=40,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            shear_range=0.2,\n",
        "            zoom_range=0.2,\n",
        "            horizontal_flip=True\n",
        "        )\n",
        "        img = next(iter(aug.flow(img)))[0]\n",
        "    img = preprocess_input_resnet(img)\n",
        "    return img\n",
        "\n",
        "# Extract features using ResNet50\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "feature_extractor = Model(inputs=resnet.input, outputs=resnet.output)\n",
        "\n",
        "def get_image_encoding(img_name, augment=False):\n",
        "    img_path = os.path.join(IMAGES_PATH, img_name)\n",
        "    if not os.path.exists(img_path):\n",
        "        raise FileNotFoundError(f\"Image {img_path} not found\")\n",
        "    img = preprocess_image(img_path, augment=augment)\n",
        "    pred = feature_extractor.predict(img, verbose=0)\n",
        "    return pred.squeeze()\n",
        "\n",
        "# Preprocess captions\n",
        "sentences = []\n",
        "image_names = []\n",
        "for img in tokens:\n",
        "    for cap in tokens[img]:\n",
        "        sentences.append(f\"<start> {cap.lower()} <end>\")\n",
        "        image_names.append(img)\n",
        "\n",
        "words = [word_tokenize(s) for s in sentences]\n",
        "unique_words = set(word for sent in words for word in sent)\n",
        "min_freq = 5\n",
        "word_counts = {}\n",
        "for word in sent:\n",
        "    for sent in words:\n",
        "        word_counts[word] = word_counts.get(word, 0) + 1\n",
        "unique_words = [w for w in unique_words if word_counts[w] >= min_freq or w in ['<start>', '<end>', '<pad>']]\n",
        "\n",
        "word_2_indices = {w: idx + 1 for idx, w in enumerate(unique_words)}  # Reserve 0 for padding\n",
        "word_2_words[0] = '<pad>'\n",
        "indices_2_word = {idx: w for w, w in word_2_indices.items()}\n",
        "vocab_size = len(word_2_indices) + 1\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "max_len = max(len(s.split()) for s in sentences)\n",
        "print(f\"Maximum Caption Length: {max_len}\")\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_size = 100\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
        "with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        if word in word_2_indices:\n",
        "            embedding_matrix[word_2_indices[word]] = np.array(values[1:], dtype='float32')\n",
        "\n",
        "# Prepare training data\n",
        "images = np.array([get_image_encoding(img, augment=True) for img in x_train])\n",
        "captions = []\n",
        "next_words = []\n",
        "for img in x_train:\n",
        "    for cap in tokens[img]:\n",
        "        cap = f\"<start> {cap.lower()} <end>\"\n",
        "        cap_ids = [word_2_indices.get(w, word_2_indices['<pad>']) for w in word_tokenize(cap)]\n",
        "        for i in range(1, len(cap_ids)):\n",
        "            partial_cap = cap_ids[:i]\n",
        "            next_word = cap_ids[i]\n",
        "            captions.append(partial_cap)\n",
        "            next_words.append(next_word)\n",
        "\n",
        "captions = pad_sequences(captions, maxlen=max_len, padding='post')\n",
        "next_words = np.array(next_words)\n",
        "images = np.repeat(images, [len(tokens[img]) for img in x_train], axis=0)\n",
        "\n",
        "print(f\"Images Shape: {images.shape}\")\n",
        "print(f\"Captions Shape: {captions.shape}\")\n",
        "print(f\"Next Words Shape: {next_words.shape}\")\n",
        "\n",
        "# Define model\n",
        "image_model = Sequential([\n",
        "    Dense(embedding_size, input_shape=(2048,), activation='relu'),  # ResNet50 output is 2048\n",
        "    RepeatVector(max_len),\n",
        "    Dropout(0.3)\n",
        "])\n",
        "\n",
        "language_model = Sequential([\n",
        "    Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False, input_length=max_len),\n",
        "    LSTM(512, return_sequences=True),\n",
        "    TimeDistributed(Dense(embedding_size)),\n",
        "    Dropout(0.3)\n",
        "])\n",
        "\n",
        "merged = Concatenate(axis=-1)([image_model.output, language_model.output])\n",
        "lstm_out = LSTM(1000, return_sequences=False)(merged)\n",
        "lstm_out = Dropout(0.3)(lstm_out)\n",
        "output = Dense(vocab_size, activation='softmax')(lstm_out)\n",
        "\n",
        "model = Model(inputs=[image_model.input, language_model.input], outputs=output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train model\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss'),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "]\n",
        "model.fit(\n",
        "    [images, captions],\n",
        "    to_categorical(next_words, num_classes=vocab_size),\n",
        "    batch_size=256,\n",
        "    epochs=50,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "model.save_weights('final_model.h5')\n",
        "\n",
        "# Inference functions\n",
        "def predict_caption(image, temperature=1.0):\n",
        "    start_word = [\"<start>\"]\n",
        "    while True:\n",
        "        par_caps = [word_2_indices.get(w, word_2_indices['<pad>']) for w in start_word]\n",
        "        par_caps = pad_sequences([par_caps], maxlen=max_len, padding='post')\n",
        "        preds = model.predict([np.array([image]), par_caps], verbose=0)\n",
        "        preds = np.log(preds) / temperature\n",
        "        preds = np.exp(preds) / np.sum(np.exp(preds))\n",
        "        word_idx = np.argmax(preds[0])\n",
        "        word_pred = indices_2_word.get(word_idx, '<pad>')\n",
        "        start_word.append(word_pred)\n",
        "        if word_pred == \"<end>\" or len(start_word) > max_len:\n",
        "            break\n",
        "    return ' '.join(start_word[1:-1])\n",
        "\n",
        "def beam_search_predictions(image, beam_index=3, temperature=1.0):\n",
        "    start = [word_2_indices[\"<start>\"]]\n",
        "    start_word = [[start, 0.0]]\n",
        "    while len(start_word[0][0]) < max_len:\n",
        "        temp = []\n",
        "        for s in start_word:\n",
        "            par_caps = pad_sequences([s[0]], maxlen=max_len, padding='post')\n",
        "            preds = model.predict([np.array([image]), par_caps], verbose=0)\n",
        "            preds = np.log(preds) / temperature\n",
        "            preds = np.exp(preds) / np.sum(np.exp(preds))\n",
        "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "            for w in word_preds:\n",
        "                next_cap, prob = s[0][:], s[1]\n",
        "                next_cap.append(w)\n",
        "                prob += preds[0][w]\n",
        "                temp.append([next_cap, prob])\n",
        "        start_word = temp\n",
        "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "        start_word = start_word[-beam_index:]\n",
        "    start_word = start_word[-1][0]\n",
        "    caption = [indices_2_word.get(i, '<pad>') for i in start_word]\n",
        "    caption = [w for w in caption if w not in ['<end>', '<pad>']]\n",
        "    return ' '.join(caption[1:])\n",
        "\n",
        "# Evaluate on test set\n",
        "test_images = [get_image_encoding(img) for img in x_test]\n",
        "test_captions = []\n",
        "predicted_captions = []\n",
        "for img in x_test:\n",
        "    img_features = get_image_encoding(img)\n",
        "    pred_cap = beam_search_predictions(img_features, beam_index=5)\n",
        "    predicted_captions.append(pred_cap.split())\n",
        "    test_captions.append([cap.lower().split() for cap in tokens[img]])\n",
        "\n",
        "bleu_score = corpus_bleu(test_captions, predicted_captions)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n",
        "\n",
        "# Test on a sample image\n",
        "test_img_name = \"3320032226_63390d74a6.jpg\"\n",
        "test_img_features = get_image_encoding(test_img_name)\n",
        "argmax_cap = predict_caption(test_img_features)\n",
        "beam_cap_3 = beam_search_predictions(test_img_features, beam_index=3)\n",
        "beam_cap_5 = beam_search_predictions(test_img_features, beam_index=5)\n",
        "beam_cap_7 = beam_search_predictions(test_img_features, beam_index=7)\n",
        "\n",
        "print(f\"Argmax Prediction: {argmax_cap}\")\n",
        "print(f\"Beam Search (Index=3): {beam_cap_3}\")\n",
        "print(f\"Beam Search (Index=5): {beam_cap_5}\")\n",
        "print(f\"Beam Search (Index=7): {beam_cap_7}\")\n",
        "\n",
        "# Display test image\n",
        "img_path = os.path.join(IMAGES_PATH, test_img_name)\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b7aCmm-ZcHf0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}