{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSRIL8ryW3DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bL72EMrwUsSX",
        "outputId": "f3952dc6-8426-46b8-9ff2-43effa269a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open P5, P5.zip or P5.ZIP.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK punkt for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "!unzip P5 Image Captioning.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set paths (adjust as needed)\n",
        "IMAGES_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flicker8k_Dataset/'\n",
        "CAPTIONS_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr8k.token.txt'\n",
        "TRAIN_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "TEST_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "GLOVE_PATH = '/data/glove.6B.100d.txt'"
      ],
      "metadata": {
        "id": "rO9OBOPzYPKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "import multiprocessing as mp\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK punkt for tokenization\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Set paths (adjust as needed)\n",
        "IMAGES_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flicker8k_Dataset/'\n",
        "CAPTIONS_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr8k.token.txt'\n",
        "TRAIN_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "TEST_PATH = '/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "GLOVE_PATH = '/data/glove.6B.100d.txt'\n",
        "\n",
        "# Load captions and create tokens dictionary\n",
        "def load_captions():\n",
        "    try:\n",
        "        captions = open(CAPTIONS_PATH, 'r').read().split(\"\\n\")\n",
        "        tokens = {}\n",
        "        for line in captions:\n",
        "            if not line:\n",
        "                continue\n",
        "            temp = line.split(\"#\")\n",
        "            if len(temp) < 2:\n",
        "                continue\n",
        "            img, cap = temp[0], temp[1][2:].lower()\n",
        "            if img in tokens:\n",
        "                tokens[img].append(cap)\n",
        "            else:\n",
        "                tokens[img] = [cap]\n",
        "        return tokens\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Captions file {CAPTIONS_PATH} not found\")\n",
        "\n",
        "tokens = load_captions()\n",
        "\n",
        "# Load train and test image names\n",
        "def load_image_lists():\n",
        "    try:\n",
        "        x_train = open(TRAIN_PATH, 'r').read().split(\"\\n\")\n",
        "        x_test = open(TEST_PATH, 'r').read().split(\"\\n\")\n",
        "        x_train = [x for x in x_train if x]\n",
        "        x_test = [x for x in x_test if x]\n",
        "        print(f\"Number of Training Images: {len(x_train)}\")\n",
        "        print(f\"Number of Test Images: {len(x_test)}\")\n",
        "        return x_train, x_test\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Train or test file not found\")\n",
        "\n",
        "x_train, x_test = load_image_lists()\n",
        "\n",
        "# Image preprocessing with augmentation\n",
        "def preprocess_image(img_path, augment=False):\n",
        "    try:\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        img = image.img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0) # Add batch dimension\n",
        "        if augment:\n",
        "            aug = ImageDataGenerator(\n",
        "                rotation_range=20,\n",
        "                width_shift_range=0.1,\n",
        "                height_shift_range=0.1,\n",
        "                shear_range=0.1,\n",
        "                zoom_range=0.1,\n",
        "                horizontal_flip=True\n",
        "            )\n",
        "            img = next(iter(aug.flow(img, batch_size=1)))[0]\n",
        "            img = np.expand_dims(img, axis=0) # Ensure batch dimension is present after augmentation\n",
        "        img = preprocess_input_resnet(img)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Feature extraction using ResNet50\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "feature_extractor = Model(inputs=resnet.input, outputs=resnet.output)\n",
        "\n",
        "def get_image_encoding(img_name, augment=False):\n",
        "    img_path = os.path.join(IMAGES_PATH, img_name)\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Image {img_path} not found\")\n",
        "        return None, img_name\n",
        "    img = preprocess_image(img_path, augment=augment)\n",
        "    if img is None:\n",
        "        return None, img_name\n",
        "    pred = feature_extractor.predict(img, verbose=0)\n",
        "    return pred, img_name # Removed .squeeze()\n",
        "\n",
        "# Parallel image encoding\n",
        "def parallel_encode_images(image_list, augment=False):\n",
        "    with mp.Pool(mp.cpu_count()) as pool:\n",
        "        results = list(tqdm(pool.starmap(get_image_encoding, [(img, augment) for img in image_list]), total=len(image_list)))\n",
        "    return {img: encoding for encoding, img in results if encoding is not None}\n",
        "\n",
        "# Encode images and save to datasets\n",
        "def encode_and_save_images(image_list, output_file, dataset_file, augment=False):\n",
        "    encoded_images = parallel_encode_images(image_list, augment=augment)\n",
        "    c_count = 0\n",
        "    with open(dataset_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"image_id\\tcaptions\\n\")\n",
        "        for img in image_list:\n",
        "            if img not in encoded_images:\n",
        "                continue\n",
        "            for capt in tokens.get(img, []):\n",
        "                caption = f\"<start> {capt} <end>\"\n",
        "                f.write(f\"{img}\\t{caption}\\n\")\n",
        "                c_count += 1\n",
        "        f.flush()\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(encoded_images, f)\n",
        "    print(f\"Saved {c_count} captions to {dataset_file}\")\n",
        "\n",
        "# Process train and test images\n",
        "encode_and_save_images(x_train, \"train_encoded_images.p\", \"flickr_8k_train_dataset.txt\", augment=True)\n",
        "encode_and_save_images(x_test, \"test_encoded_images.p\", \"flickr_8k_val_dataset.txt\", augment=False)\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocabulary(dataset_file):\n",
        "    pd_dataset = pd.read_csv(dataset_file, delimiter='\\t')\n",
        "    ds = pd_dataset.values\n",
        "    print(f\"Dataset Shape: {ds.shape}\")\n",
        "\n",
        "    sentences = [row[1] for row in ds if isinstance(row[1], str)]\n",
        "    print(f\"Number of Sentences: {len(sentences)}\")\n",
        "\n",
        "    words = [word_tokenize(sent) for sent in sentences]\n",
        "    word_counts = {}\n",
        "    for sent in words:\n",
        "        for word in sent:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "    min_freq = 5\n",
        "    unique_words = [w for w in set(word for sent in words for word in sent)\n",
        "                    if word_counts[w] >= min_freq or w in ['<start>', '<end>', '<pad>']]\n",
        "    print(f\"Vocabulary Size (after filtering): {len(unique_words)}\")\n",
        "\n",
        "    word_2_indices = {w: idx + 1 for idx, w in enumerate(unique_words)}  # Reserve 0 for padding\n",
        "    word_2_indices['<pad>'] = 0\n",
        "    indices_2_word = {idx: w for w, idx in word_2_indices.items()}\n",
        "\n",
        "    print(f\"Index of <start>: {word_2_indices['<start>']}\")\n",
        "    print(f\"Word at index {word_2_indices['<start>']}: {indices_2_word[word_2_indices['<start>']]}\")\n",
        "\n",
        "    return sentences, word_2_indices, indices_2_word, len(unique_words) + 1\n",
        "\n",
        "sentences, word_2_indices, indices_2_word, vocab_size = build_vocabulary(\"flickr_8k_train_dataset.txt\")\n",
        "\n",
        "# Compute maximum caption length\n",
        "max_len = max(len(word_tokenize(sent)) for sent in sentences)\n",
        "print(f\"Maximum Caption Length: {max_len}\")\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_size = 100\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
        "try:\n",
        "    with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if word in word_2_indices:\n",
        "                embedding_matrix[word_2_indices[word]] = np.array(values[1:], dtype='float32')\n",
        "    np.save(\"embedding_matrix.npy\", embedding_matrix)\n",
        "except FileNotFoundError:\n",
        "    print(f\"GloVe file {GLOVE_PATH} not found. Embedding matrix will contain zeros.\")\n",
        "\n",
        "# Create padded sequences and next words\n",
        "def create_sequences(sentences, word_2_indices, max_len, vocab_size):\n",
        "    padded_sequences = []\n",
        "    subsequent_words = []\n",
        "    for sent in sentences:\n",
        "        text = word_tokenize(sent)\n",
        "        text_ids = [word_2_indices.get(word, word_2_indices['<pad>']) for word in text]\n",
        "        for i in range(1, len(text_ids)):\n",
        "            padded_seq = text_ids[:i]\n",
        "            next_word = text_ids[i]\n",
        "            padded_seq = pad_sequences([padded_seq], maxlen=max_len, padding='post')[0]\n",
        "            subsequent_words.append(next_word)  # Store index instead of one-hot\n",
        "    return np.array(padded_sequences), np.array(subsequent_words)\n",
        "\n",
        "padded_sequences, subsequent_words = create_sequences(sentences, word_2_indices, max_len, vocab_size)\n",
        "print(f\"Padded Sequences Shape: {padded_sequences.shape}\")\n",
        "print(f\"Subsequent Words Shape: {subsequent_words.shape}\")\n",
        "\n",
        "# Load encoded images\n",
        "with open('train_encoded_images.p', 'rb') as f:\n",
        "    encoded_images = pickle.load(f)\n",
        "\n",
        "# Create image array\n",
        "ds = pd.read_csv(\"flickr_8k_train_dataset.txt\", delimiter='\\t').values\n",
        "imgs = np.array([encoded_images[row[0]] for row in ds if row[0] in encoded_images])\n",
        "print(f\"Images Shape: {imgs.shape}\")\n",
        "\n",
        "# Limit to a subset for faster processing\n",
        "number_of_images = 1500\n",
        "captions = []\n",
        "next_words = []\n",
        "images = []\n",
        "image_names = []\n",
        "\n",
        "for ix in range(min(number_of_images, len(sentences))):\n",
        "    text = word_tokenize(sentences[ix])\n",
        "    text_ids = [word_2_indices.get(word, word_2_indices['<pad>']) for word in text]\n",
        "    for i in range(1, len(text_ids)):\n",
        "        padded_seq = text_ids[:i]\n",
        "        next_word = text_ids[i]\n",
        "        padded_seq = pad_sequences([padded_seq], maxlen=max_len, padding='post')[0]\n",
        "        captions.append(padded_seq)\n",
        "        next_words.append(next_word)\n",
        "        # Use the image feature corresponding to the current image\n",
        "        images.append(imgs[ix])\n",
        "        image_names.append(ds[ix, 0])\n",
        "\n",
        "captions = np.array(captions)\n",
        "next_words = np.array(next_words)\n",
        "images = np.array(images)\n",
        "image_names = np.array(image_names)\n",
        "\n",
        "\n",
        "# Save processed data\n",
        "np.save(\"captions.npy\", captions)\n",
        "np.save(\"next_words.npy\", next_words)\n",
        "np.save(\"images.npy\", images)\n",
        "np.save(\"image_names.npy\", image_names)\n",
        "\n",
        "print(f\"Final Captions Shape: {captions.shape}\")\n",
        "print(f\"Final Next Words Shape: {next_words.shape}\")\n",
        "print(f\"Final Images Shape: {images.shape}\")\n",
        "print(f\"Final Image Names Length: {len(image_names)}\")\n",
        "\n",
        "# Save vocabulary for later use\n",
        "with open(\"word_2_indices.p\", \"wb\") as f:\n",
        "    pickle.dump(word_2_indices, f)\n",
        "with open(\"indices_2_word.p\", \"wb\") as f:\n",
        "    pickle.dump(indices_2_word, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTFC7HKrmFP4",
        "outputId": "07287f29-d923-4a47-ef7a-43d0ea70c655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Images: 6000\n",
            "Number of Test Images: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW4o4eiYmSm2",
        "outputId": "4915d844-8b4f-49d0-e2dc-7e90481cde81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as preprocess_input_resnet\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model # Import Model class\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK punkt for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Load captions and create tokens dictionary\n",
        "def load_captions():\n",
        "    try:\n",
        "        captions = open(CAPTIONS_PATH, 'r').read().split(\"\\n\")\n",
        "        tokens = {}\n",
        "        for line in captions:\n",
        "            if not line:\n",
        "                continue\n",
        "            temp = line.split(\"#\")\n",
        "            if len(temp) < 2:\n",
        "                continue\n",
        "            img, cap = temp[0], temp[1][2:].lower()\n",
        "            if img in tokens:\n",
        "                tokens[img].append(cap)\n",
        "            else:\n",
        "                tokens[img] = [cap]\n",
        "        return tokens\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Captions file {CAPTIONS_PATH} not found\")\n",
        "\n",
        "tokens = load_captions()\n",
        "\n",
        "# Load train and test image names\n",
        "def load_image_lists():\n",
        "    try:\n",
        "        x_train = open(TRAIN_PATH, 'r').read().split(\"\\n\")\n",
        "        x_test = open(TEST_PATH, 'r').read().split(\"\\n\")\n",
        "        x_train = [x for x in x_train if x]\n",
        "        x_test = [x for x in x_test if x]\n",
        "        print(f\"Number of Training Images: {len(x_train)}\")\n",
        "        print(f\"Number of Test Images: {len(x_test)}\")\n",
        "        return x_train, x_test\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Train or test file not found\")\n",
        "\n",
        "x_train, x_test = load_image_lists()\n",
        "\n",
        "# Image preprocessing with augmentation\n",
        "def preprocess_image(img_path, augment=False):\n",
        "    try:\n",
        "        img = image.load_img(img_path, target_size=(224, 224))\n",
        "        img = image.img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        if augment:\n",
        "            aug = ImageDataGenerator(\n",
        "                rotation_range=20,\n",
        "                width_shift_range=0.1,\n",
        "                height_shift_range=0.1,\n",
        "                shear_range=0.1,\n",
        "                zoom_range=0.1,\n",
        "                horizontal_flip=True\n",
        "            )\n",
        "            img = next(iter(aug.flow(img)))[0]\n",
        "            img = np.expand_dims(img, axis=0) # Add back the batch dimension after augmentation\n",
        "        img = preprocess_input_resnet(img)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Feature extraction using ResNet50\n",
        "resnet = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "feature_extractor = Model(inputs=resnet.input, outputs=resnet.output)\n",
        "\n",
        "def get_image_encoding(img_name, augment=False):\n",
        "    img_path = os.path.join(IMAGES_PATH, img_name)\n",
        "    if not os.path.exists(img_path):\n",
        "        print(f\"Image {img_path} not found\")\n",
        "        return None\n",
        "    img = preprocess_image(img_path, augment=augment)\n",
        "    if img is None:\n",
        "        return None\n",
        "    pred = feature_extractor.predict(img, verbose=0)\n",
        "    return pred # Removed .squeeze()\n",
        "\n",
        "# Encode images and save to datasets\n",
        "def encode_and_save_images(image_list, output_file, dataset_file, augment=False):\n",
        "    encoded_images = {}\n",
        "    c_count = 0\n",
        "    with open(dataset_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"image_id\\tcaptions\\n\")\n",
        "        for img in image_list:\n",
        "            encoding = get_image_encoding(img, augment=augment)\n",
        "            if encoding is None:\n",
        "                continue\n",
        "            encoded_images[img] = encoding\n",
        "            for capt in tokens.get(img, []):\n",
        "                caption = f\"<start> {capt} <end>\"\n",
        "                f.write(f\"{img}\\t{caption}\\n\")\n",
        "                c_count += 1\n",
        "        f.flush()\n",
        "    with open(output_file, 'wb') as f:\n",
        "        pickle.dump(encoded_images, f)\n",
        "    print(f\"Saved {c_count} captions to {dataset_file}\")\n",
        "\n",
        "# Process train and test images\n",
        "encode_and_save_images(x_train, \"train_encoded_images.p\", \"flickr_8k_train_dataset.txt\", augment=True)\n",
        "encode_and_save_images(x_test, \"test_encoded_images.p\", \"flickr_8k_val_dataset.txt\", augment=False)\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocabulary(dataset_file):\n",
        "    pd_dataset = pd.read_csv(dataset_file, delimiter='\\t')\n",
        "    ds = pd_dataset.values\n",
        "    print(f\"Dataset Shape: {ds.shape}\")\n",
        "\n",
        "    sentences = [row[1] for row in ds if isinstance(row[1], str)]\n",
        "    print(f\"Number of Sentences: {len(sentences)}\")\n",
        "\n",
        "    words = [word_tokenize(sent) for sent in sentences]\n",
        "    word_counts = {}\n",
        "    for sent in words:\n",
        "        for word in sent:\n",
        "            word_counts[word] = word_counts.get(word, 0) + 1\n",
        "\n",
        "    min_freq = 5\n",
        "    unique_words = [w for w in set(word for sent in words for word in sent)\n",
        "                    if word_counts[w] >= min_freq or w in ['<start>', '<end>', '<pad>']]\n",
        "    print(f\"Vocabulary Size (after filtering): {len(unique_words)}\")\n",
        "\n",
        "    word_2_indices = {w: idx + 1 for idx, w in enumerate(unique_words)}  # Reserve 0 for padding\n",
        "    word_2_indices['<pad>'] = 0\n",
        "    indices_2_word = {idx: w for w, idx in word_2_indices.items()}\n",
        "\n",
        "    print(f\"Index of <start>: {word_2_indices['<start>']}\")\n",
        "    print(f\"Word at index {word_2_indices['<start>']}: {indices_2_word[word_2_indices['<start>']]}\")\n",
        "\n",
        "    return sentences, word_2_indices, indices_2_word, len(unique_words) + 1\n",
        "\n",
        "sentences, word_2_indices, indices_2_word, vocab_size = build_vocabulary(\"flickr_8k_train_dataset.txt\")\n",
        "\n",
        "# Compute maximum caption length\n",
        "max_len = max(len(word_tokenize(sent)) for sent in sentences)\n",
        "print(f\"Maximum Caption Length: {max_len}\")\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_size = 100\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
        "with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        if word in word_2_indices:\n",
        "            embedding_matrix[word_2_indices[word]] = np.array(values[1:], dtype='float32')\n",
        "np.save(\"embedding_matrix.npy\", embedding_matrix)\n",
        "\n",
        "# Create padded sequences and next words\n",
        "def create_sequences(sentences, word_2_indices, max_len, vocab_size):\n",
        "    padded_sequences = []\n",
        "    subsequent_words = []\n",
        "    for sent in sentences:\n",
        "        text = word_tokenize(sent)\n",
        "        text_ids = [word_2_indices.get(word, word_2_indices['<pad>']) for word in text]\n",
        "        for i in range(1, len(text_ids)):\n",
        "            padded_seq = text_ids[:i]\n",
        "            next_word = text_ids[i]\n",
        "            padded_seq = pad_sequences([padded_seq], maxlen=max_len, padding='post')[0]\n",
        "            next_word_1hot = np.zeros(vocab_size, dtype=np.bool_)\n",
        "            next_word_1hot[next_word] = 1\n",
        "            padded_sequences.append(padded_seq)\n",
        "            subsequent_words.append(next_word_1hot)\n",
        "\n",
        "    return np.array(padded_sequences), np.array(subsequent_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "N-0gMlAZXBLl",
        "outputId": "070322b5-42f5-442b-8ad9-8f236d94706e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Training Images: 6000\n",
            "Number of Test Images: 1000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-2263335476.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;31m# Process train and test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mencode_and_save_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_encoded_images.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flickr_8k_train_dataset.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0mencode_and_save_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_encoded_images.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flickr_8k_val_dataset.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-22-2263335476.py\u001b[0m in \u001b[0;36mencode_and_save_images\u001b[0;34m(image_list, output_file, dataset_file, augment)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image_id\\tcaptions\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-22-2263335476.py\u001b[0m in \u001b[0;36mget_image_encoding\u001b[0;34m(img_name, augment)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;31m# Removed .squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    498\u001b[0m     ):\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoShardPolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwith_options\u001b[0;34m(self, options, name)\u001b[0m\n\u001b[1;32m   3010\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0man\u001b[0m \u001b[0moption\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0monce\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3011\u001b[0m     \"\"\"\n\u001b[0;32m-> 3012\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_OptionsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, options, name)\u001b[0m\n\u001b[1;32m   4905\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4906\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4907\u001b[0;31m       variant_tensor = gen_dataset_ops.options_dataset(\n\u001b[0m\u001b[1;32m   4908\u001b[0m           \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions_pb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4909\u001b[0m           **self._common_args)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptions_dataset\u001b[0;34m(input_dataset, serialized_options, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   4661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4662\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4663\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   4664\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OptionsDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serialized_options\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4665\u001b[0m         \u001b[0mserialized_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences, subsequent_words = create_sequences(sentences, word_2_indices, max_len, vocab_size)\n",
        "print(f\"Padded Sequences Shape: {padded_sequences.shape}\")\n",
        "print(f\"Subsequent Words Shape: {subsequent_words.shape}\")\n",
        "\n",
        "# Load encoded images\n",
        "with open('train_encoded_images.p', 'rb') as f:\n",
        "    encoded_images = pickle.load(f)\n",
        "\n",
        "# Create image array\n",
        "ds = pd.read_csv(\"/content/drive/MyDrive/P5 Image Captioning/Flickr8k_text/Flickr_8k.trainImages.txt\", delimiter='\\t').values\n",
        "imgs = np.array([encoded_images[row[0]] for row in ds if row[0] in encoded_images])\n",
        "print(f\"Images Shape: {imgs.shape}\")\n",
        "\n",
        "# Limit to a subset for faster processing (adjust as needed)\n",
        "number_of_images = 1500\n",
        "captions = np.zeros([0, max_len])\n",
        "next_words = np.zeros([0, vocab_size])\n",
        "images = []\n",
        "image_names = []\n",
        "\n",
        "for ix in range(min(number_of_images, len(sentences))):\n",
        "    captions = np.concatenate([captions, padded_sequences[ix:ix+1]])\n",
        "    next_words = np.concatenate([next_words, subsequent_words[ix:ix+1]])\n",
        "    for _ in range(len(padded_sequences[ix])):\n",
        "        images.append(imgs[ix])\n",
        "        image_names.append(ds[ix, 0])\n",
        "\n",
        "captions = np.array(captions)\n",
        "next_words = np.array(next_words)\n",
        "images = np.array(images)\n",
        "image_names = np.array(image_names)\n",
        "\n",
        "# Save processed data\n",
        "np.save(\"captions.npy\", captions)\n",
        "np.save(\"next_words.npy\", next_words)\n",
        "np.save(\"images.npy\", images)\n",
        "np.save(\"image_names.npy\", image_names)\n",
        "\n",
        "print(f\"Final Captions Shape: {captions.shape}\")\n",
        "print(f\"Final Next Words Shape: {next_words.shape}\")\n",
        "print(f\"Final Images Shape: {images.shape}\")\n",
        "print(f\"Final Image Names Length: {len(image_names)}\")\n",
        "\n",
        "# Save vocabulary for later use\n",
        "with open(\"word_2_indices.p\", \"wb\") as f:\n",
        "    pickle.dump(word_2_indices, f)\n",
        "with open(\"indices_2_word.p\", \"wb\") as f:\n",
        "    pickle.dump(indices_2_word, f)"
      ],
      "metadata": {
        "id": "XppbHddSY1z-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}